{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mubarak-exe/Fake-News-Detection-using-Hybrid-BiLSTM-TCN-Model-with-Attention-Mechanism/blob/main/BLSTM_ATT_TCN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hS5VEmQ8om32"
      },
      "source": [
        "# Importing the required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVFjXo_xoYc-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from tensorflow import expand_dims\n",
        "from tensorflow.math import reduce_sum\n",
        "from tensorflow.nn import tanh, softmax\n",
        "from tensorflow.keras.layers import Dense, Embedding, Conv1D, MaxPool1D, Input, LSTM, Bidirectional, Layer,Dot, Multiply, Dropout\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "# from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.dtypes import uint8, float32\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSLej1oL8Pnd"
      },
      "outputs": [],
      "source": [
        "!pip install -q keras-tcn\n",
        "from tcn import TCN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHEqV73jotZu"
      },
      "outputs": [],
      "source": [
        "from tensorflow.random import set_seed\n",
        "set_seed(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nx7QnXUJpCLs"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apidnb2bow9V"
      },
      "source": [
        "# Data Loading and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sIhNiGsOo04H"
      },
      "outputs": [],
      "source": [
        "true = pd.read_csv('/content/drive/MyDrive/data_set_1/ISOT Fake News Dataset/True.csv')\n",
        "fake = pd.read_csv('/content/drive/MyDrive/data_set_1/ISOT Fake News Dataset/Fake.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMIo0V5ppJ5G"
      },
      "outputs": [],
      "source": [
        "# add 1 for label for true and 0 fro fake\n",
        "true[\"label\"] = 1\n",
        "fake['label'] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zAOdOBtPpOLm"
      },
      "outputs": [],
      "source": [
        "# Combine both dataframes and shuffle\n",
        "input_data = pd.concat( [true,fake] )\n",
        "input_data = input_data.sample(frac = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amvJHd4dpTj0"
      },
      "outputs": [],
      "source": [
        "# remove website url and ip\n",
        "input_data['text']= input_data['text'].apply(lambda x: re.sub(r\"(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})\", \"\", x))\n",
        "input_data['text']= input_data['text'].apply(lambda x: re.sub(r\"^(?!mailto:)(?:(?:http|https|ftp)://)(?:\\\\S+(?::\\\\S*)?@)?(?:(?:(?:[1-9]\\\\d?|1\\\\d\\\\d|2[01]\\\\d|22[0-3])(?:\\\\.(?:1?\\\\d{1,2}|2[0-4]\\\\d|25[0-5])){2}(?:\\\\.(?:[0-9]\\\\d?|1\\\\d\\\\d|2[0-4]\\\\d|25[0-4]))|(?:(?:[a-z\\\\u00a1-\\\\uffff0-9]+-?)*[a-z\\\\u00a1-\\\\uffff0-9]+)(?:\\\\.(?:[a-z\\\\u00a1-\\\\uffff0-9]+-?)*[a-z\\\\u00a1-\\\\uffff0-9]+)*(?:\\\\.(?:[a-z\\\\u00a1-\\\\uffff]{2,})))|localhost)(?::\\\\d{2,5})?(?:(/|\\\\?|#)[^\\\\s]*)?$\", \"\", x))\n",
        "input_data['text']= input_data['text'].apply(lambda x: re.sub(r\"^((25[0-5]|(2[0-4]|1[0-9]|[1-9]|)[0-9])(\\.(?!$)|$)){4}$\", \"\", x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1M3WtwWqpXRb"
      },
      "outputs": [],
      "source": [
        "# Remove Stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "stopwords=stopwords.words('english')\n",
        "input_data['text'] = input_data['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93fQXH4rpmnM"
      },
      "outputs": [],
      "source": [
        "#STEMMING\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import pandas as pd\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBwYPll_pqv1"
      },
      "outputs": [],
      "source": [
        "porter = PorterStemmer()\n",
        "# for word in input_data['text']:\n",
        "#     print(porter.stem(word))\n",
        "input_data['text'] = input_data['text'].apply(lambda x: ' '.join([porter.stem(y) for y in x.split()]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Douzj04tfoM"
      },
      "source": [
        "Mapping Text to Vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCDj8me_fu9S"
      },
      "outputs": [],
      "source": [
        "pip install keras-preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wH0o_UxcpteF"
      },
      "outputs": [],
      "source": [
        "# Tockenization\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.layers import Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QHcxqeipw5t"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(num_words=9999999999)\n",
        "tokenizer.fit_on_texts(input_data['text'])\n",
        "sequences = tokenizer.texts_to_sequences(input_data['text'])\n",
        "word_index = tokenizer.word_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPpF7YFDp0Fs"
      },
      "outputs": [],
      "source": [
        "len(sequences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajmP7OZQrsHF"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "sequences=tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    sequences,\n",
        "    maxlen=100,\n",
        "    dtype='int32',\n",
        "    padding='post',\n",
        "    truncating='pre',\n",
        "    value=0.0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tAabIoZ-DSSq"
      },
      "outputs": [],
      "source": [
        "sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2onaipNtoj-"
      },
      "outputs": [],
      "source": [
        "GLOVE_DIR = \"data\"\n",
        "embeddings_index = {}\n",
        "f = open(os.path.join(GLOVE_DIR, '/content/drive/MyDrive/data_set_1/ISOT Fake News Dataset/glove.6B.300d.txt'), encoding=\"utf8\")\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    #print(values[1:])\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Total %s word vectors in Glove.' % len(embeddings_index))\n",
        "\n",
        "embedding_matrix = np.random.random((len(word_index) + 1, 300))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "embedding_layer = Embedding(len(word_index) + 1,\n",
        "                            300,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=300)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4-W2TvNTqEs"
      },
      "source": [
        "# SPLITTING THE DATA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53PWbQDdtuPl"
      },
      "outputs": [],
      "source": [
        "data=sequences\n",
        "label= input_data[\"label\"]\n",
        "x_train, x_test, y_train, y_test = train_test_split( data, label, test_size=0.20, random_state=42)\n",
        "x_test, x_val, y_test, y_val = train_test_split( x_test, y_test, test_size=0.50, random_state=42)\n",
        "print('Size of train, validation, test:', len(y_train), len(y_val), len(y_test))\n",
        "\n",
        "print('real & fake news in train,valt,test:')\n",
        "print(y_train.sum(axis=0))\n",
        "print(y_val.sum(axis=0))\n",
        "print(y_test.sum(axis=0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8T3ncgAUCcE"
      },
      "source": [
        "# MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BjclOqGT32uN"
      },
      "outputs": [],
      "source": [
        "path = '/content/drive/MyDrive/Colab Notebooks/Project_Models/Processed_Data/'\n",
        "path1 = '/content/drive/MyDrive/Colab Notebooks/Project_Models/'\n",
        "x_train=pickle.load(open(path+'x_train.pkl', 'rb'))\n",
        "y_train=pickle.load(open(path+'y_train.pkl', 'rb'))\n",
        "y_test=pickle.load(open(path+'y_test.pkl', 'rb'))\n",
        "x_test=pickle.load(open(path+'x_test.pkl', 'rb'))\n",
        "x_val=pickle.load(open(path+'x_val.pkl', 'rb'))\n",
        "y_val=pickle.load(open(path+'y_val.pkl', 'rb'))\n",
        "embedding_layer = pickle.load(open(path1+'i100embedding_layer.pkl', 'rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGW815Pvt1pV"
      },
      "outputs": [],
      "source": [
        "class Attention(Layer):\n",
        "\n",
        "    def __init__(self, return_sequences=True):\n",
        "        self.return_sequences = return_sequences\n",
        "        super(Attention,self).__init__()\n",
        "\n",
        "    def build(self, input_shape):\n",
        "\n",
        "        self.W=self.add_weight(name=\"att_weight\", shape=(input_shape[-1],1),\n",
        "                               initializer=\"normal\")\n",
        "        self.b=self.add_weight(name=\"att_bias\", shape=(input_shape[1],1),\n",
        "                               initializer=\"zeros\")\n",
        "\n",
        "        super(Attention,self).build(input_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "\n",
        "        w = expand_dims(self.W, 0)\n",
        "        e = tanh(Dot(axes = [2, 1])([x,w])+self.b)\n",
        "        a = softmax(e, axis=1)\n",
        "        output = x*a\n",
        "\n",
        "        if self.return_sequences:\n",
        "            return output\n",
        "\n",
        "        return reduce_sum(output, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SxnyTEM3tz8M"
      },
      "outputs": [],
      "source": [
        "i = Input([100], dtype=uint8)\n",
        "x = embedding_layer(i)\n",
        "max_len = 200\n",
        "rnn_cell_size = 128\n",
        "vocab_size = 250\n",
        "\n",
        "x = Bidirectional(LSTM(rnn_cell_size,\n",
        "                        return_sequences=True), name=\"bi_lstm_0\")(x)\n",
        "x= Dropout(0.30)(x)\n",
        "x = Attention(return_sequences=True)(x)\n",
        "x = TCN(return_sequences=False)(x)\n",
        "output = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model = Model(inputs=i , outputs=output)\n",
        "model.compile(optimizer=Adam(),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9GBvphhKlPp"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2sEZkSgWsEC"
      },
      "outputs": [],
      "source": [
        "class myCallback(Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    acc = logs.get('accuracy')\n",
        "    val_acc = logs.get('val_accuracy')\n",
        "    if (epoch % 5 == 0) and (epoch != 0):\n",
        "      model_name = f'BLSTM-ATT-TCN_e{epoch}'  # add model name (name_) as required\n",
        "      model_path = '/content/drive/MyDrive/Colab Notebooks/Project_Models/BLSTM_ATT_TCN_Models'  # add model path as required\n",
        "      self.model.save(os.path.join(model_path, model_name))\n",
        "\n",
        "callback = myCallback()\n",
        "\n",
        "callbacks=[callback]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mixnl7gruSYO"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "EPOCHS = 40\n",
        "TRAINING_STEPS = len(x_train) //  BATCH_SIZE\n",
        "VALIDATION_STEPS = len(x_val) // BATCH_SIZE\n",
        "\n",
        "history = model.fit(x_train,y_train,\n",
        "                    steps_per_epoch= TRAINING_STEPS,\n",
        "                    validation_data=[x_val,y_val],\n",
        "                    validation_steps=VALIDATION_STEPS,\n",
        "                    epochs=EPOCHS,\n",
        "                    callbacks=[callback],\n",
        "                    verbose='auto')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9qxLOThGfcRJ"
      },
      "outputs": [],
      "source": [
        "path = '/content/drive/MyDrive/Colab Notebooks/Project_Models/Processed_Data/'\n",
        "pickle.dump(history, open(path+'history_BLSTM_ATT_TCN.pkl', 'wb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROe5eqVjUUA1"
      },
      "source": [
        "# Metrics and Graphs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5o4YZwRlGKK"
      },
      "outputs": [],
      "source": [
        "# Training History\n",
        "print(history.history.keys())\n",
        "# summarize history for accuracy\n",
        "\n",
        "plt.plot(history.history['val_accuracy'], )\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1PTD3OjJsCB"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "agmSrGlBJr_J"
      },
      "outputs": [],
      "source": [
        "model=load_model('/content/drive/MyDrive/Colab Notebooks/Project_Models/BLSTM_ATT_TCN_Models/BLSTM-ATT-TCN_e35')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hdjt2jiGJr4j"
      },
      "outputs": [],
      "source": [
        "y_pred=model.predict(x_test)\n",
        "y_pred = np.squeeze(y_pred)\n",
        "y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FB5ifjA8J3Za"
      },
      "outputs": [],
      "source": [
        "p = lambda t : 1 if t>=0.5 else 0\n",
        "y_pred=np.vectorize(p)(y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q58brjUsJ6Lt"
      },
      "outputs": [],
      "source": [
        "conf_matrix = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
        "fig, ax = plt.subplots(figsize=(5, 5))\n",
        "ax.matshow(conf_matrix, cmap=plt.cm.Oranges, alpha=0.3)\n",
        "for i in range(conf_matrix.shape[0]):\n",
        "    for j in range(conf_matrix.shape[1]):\n",
        "        ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center', size='xx-large')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwS5R3KaL_GC"
      },
      "outputs": [],
      "source": [
        "print('Accuracy: %.3f' % accuracy_score(y_test, y_pred))\n",
        "print('Precision: %.3f' % precision_score(y_test, y_pred))\n",
        "print('Recall: %.3f' % recall_score(y_test, y_pred))\n",
        "print('F1 Score: %.3f' % f1_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8df0c3vOQ5Tg"
      },
      "outputs": [],
      "source": [
        "model.evaluate(x_test, y_test, verbose = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "onlnxxVE-0Bx"
      },
      "outputs": [],
      "source": [
        "dot_img_file = '/tmp/model_1.png'\n",
        "tf.keras.utils.plot_model(model, to_file=dot_img_file, show_shapes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxncy7ABntTH"
      },
      "outputs": [],
      "source": [
        "accuracy=[]\n",
        "val_accuracy=[]\n",
        "loss=[]\n",
        "val_loss=[]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwdPmmldqcTQ"
      },
      "outputs": [],
      "source": [
        "final=['history_CNN_LSTM.pkl', 'history_CNN_BLSTM_ATT.pkl', 'history_BLSTM_ATT_TCN.pkl']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hu3g5m3cnvF3"
      },
      "outputs": [],
      "source": [
        "from nltk.tag.hunpos import find_binary\n",
        "for x in final:\n",
        "              history=pickle.load(open(path1+x, 'rb'))\n",
        "              accuracy.append(history.history['accuracy'])\n",
        "              val_accuracy.append(history.history['val_accuracy'])\n",
        "              loss.append(history.history['loss'])\n",
        "              val_loss.append(history.history['val_loss'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fq60KzKHy9Mm"
      },
      "outputs": [],
      "source": [
        "print (*accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kz2DsXI3Z-ZG"
      },
      "outputs": [],
      "source": [
        "for x in accuracy:\n",
        "                  plt.plot(x)\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['Model 1', 'Model 2','Model 3'], loc='lower right')\n",
        "f = plt.figure()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fH7tQj8C2nQ-"
      },
      "outputs": [],
      "source": [
        "for x in val_accuracy:\n",
        "                  plt.plot(x)\n",
        "plt.ylabel('val_accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['Model1', 'Model2','Model3','Model4'], loc='lower rightt')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kGgoM3C2wUF"
      },
      "outputs": [],
      "source": [
        "for x in loss:\n",
        "                  plt.plot(x)\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['Model1', 'Model2','Model3','Model4'], loc='upper right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "daUbgr7b21bH"
      },
      "outputs": [],
      "source": [
        "for x in val_loss:\n",
        "                  plt.plot(x)\n",
        "plt.ylabel('val_loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['Model1', 'Model2','Model3','Model4'], loc='upper right')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "h4-W2TvNTqEs"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}